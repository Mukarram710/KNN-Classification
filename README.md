# KNN-Classification
My 6th task in AI/ML Internship

# ğŸŒ¸ K-Nearest Neighbors (KNN) Classifier on Iris Dataset

My  for the AI & ML Internship! ğŸš€  
This project implements the **KNN Classification Algorithm** using the classic **Iris dataset** ğŸŒ¼.

---

## ğŸ§  What I Learned

- ğŸ“Œ **Instance-based Learning** â€” How KNN stores and compares examples rather than learning weights.
- ğŸ“ **Distance Metrics** â€” How Euclidean distance is used to find the nearest neighbors.
- ğŸ” **Effect of K** â€” How changing the value of `K` affects accuracy and model behavior.
- ğŸ¨ **Visualization** â€” Used PCA to reduce dimensions and plot decision boundaries.
- ğŸ§¼ **Importance of Normalization** â€” Scaling features improves KNN's performance.

---

## ğŸ› ï¸ Tools & Libraries

- ğŸ Python  
- ğŸ“Š Pandas & NumPy  
- ğŸ“‰ Scikit-learn for modeling & preprocessing  
- ğŸ–¼ï¸ Matplotlib & Seaborn for visualization

---

## ğŸ“ˆ Results

- Explored multiple `K` values from 1 to 10 and plotted accuracy.
- Found optimal `K` and evaluated the final model using:
  - âœ… **Accuracy Score**
  - ğŸ”„ **Confusion Matrix**
  - ğŸ—ºï¸ **Decision Boundary (using PCA)**

---

## ğŸ’¡ Key Takeaways

- KNN is simple but powerful for classification tasks.
- Choosing the right `K` is critical â€” too small causes overfitting, too large causes underfitting.
- Visualizations make model behavior easier to interpret.

