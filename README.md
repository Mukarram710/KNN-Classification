# KNN-Classification
My 6th task in AI/ML Internship

# 🌸 K-Nearest Neighbors (KNN) Classifier on Iris Dataset

My  for the AI & ML Internship! 🚀  
This project implements the **KNN Classification Algorithm** using the classic **Iris dataset** 🌼.

---

## 🧠 What I Learned

- 📌 **Instance-based Learning** — How KNN stores and compares examples rather than learning weights.
- 📏 **Distance Metrics** — How Euclidean distance is used to find the nearest neighbors.
- 🔍 **Effect of K** — How changing the value of `K` affects accuracy and model behavior.
- 🎨 **Visualization** — Used PCA to reduce dimensions and plot decision boundaries.
- 🧼 **Importance of Normalization** — Scaling features improves KNN's performance.

---

## 🛠️ Tools & Libraries

- 🐍 Python  
- 📊 Pandas & NumPy  
- 📉 Scikit-learn for modeling & preprocessing  
- 🖼️ Matplotlib & Seaborn for visualization

---

## 📈 Results

- Explored multiple `K` values from 1 to 10 and plotted accuracy.
- Found optimal `K` and evaluated the final model using:
  - ✅ **Accuracy Score**
  - 🔄 **Confusion Matrix**
  - 🗺️ **Decision Boundary (using PCA)**

---

## 💡 Key Takeaways

- KNN is simple but powerful for classification tasks.
- Choosing the right `K` is critical — too small causes overfitting, too large causes underfitting.
- Visualizations make model behavior easier to interpret.

